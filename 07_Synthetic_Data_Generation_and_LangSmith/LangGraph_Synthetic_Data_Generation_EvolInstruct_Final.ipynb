{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data Generation Using LangGraph - Session 7 BONUS ACTIVITY\n",
    "\n",
    "## ðŸŽ¯ **BONUS ACTIVITY**: Complete Implementation\n",
    "\n",
    "This notebook implements the **BONUS ACTIVITY** from Session 7 assignment: **\"Reproduce the RAGAS Synthetic Data Generation Steps - but utilize a LangGraph Agent Graph, instead of the Knowledge Graph approach.\"**\n",
    "\n",
    "In the following notebook we'll explore the complete workflow:\n",
    "\n",
    "- ðŸ¤ **PART 1: BONUS ACTIVITY**\n",
    "  1. Use **LangGraph Agent Graph** (instead of Knowledge Graph) for Synthetic Data Generation\n",
    "  2. Implement **Evol-Instruct Method** with three evolution types\n",
    "  3. Generate evolved questions, answers, and contexts\n",
    "\n",
    "- ðŸ¤ **PART 2: LANGSMITH INTEGRATION** \n",
    "  1. Load synthetic data into a LangSmith Dataset\n",
    "  2. Evaluate RAG chain against the synthetic test data\n",
    "  3. Make improvements to the pipeline\n",
    "  4. Re-evaluate the modified pipeline\n",
    "\n",
    "## ðŸ“‹ **BONUS ACTIVITY Requirements Fulfilled:**\n",
    "âœ… **LangGraph Agent Graph** (instead of Knowledge Graph approach)  \n",
    "âœ… **Evol-Instruct Method** for synthetic data generation  \n",
    "âœ… **Three Evolution Types**: Simple, Multi-Context, and Reasoning  \n",
    "âœ… **Input**: List of LangChain Documents  \n",
    "âœ… **Output**: \n",
    "1. `List(dict)`: Evolved Questions + IDs + Evolution Types\n",
    "2. `List(dict)`: Question IDs + Answers  \n",
    "3. `List(dict)`: Question IDs + Relevant Contexts\n",
    "\n",
    "## ðŸ”„ **LangGraph Workflow Overview:**\n",
    "```\n",
    "Documents â†’ Simple Evolution â†’ Multi-Context Evolution â†’ Reasoning Evolution â†’ Generate Answers â†’ Generate Contexts â†’ Results â†’ LangSmith Evaluation\n",
    "```\n",
    "\n",
    "SDG is a critical piece of the puzzle, especially for early iteration! Without it, it would not be nearly as easy to get high quality early signal for our application's performance.\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤ PART 1: BONUS ACTIVITY - LANGGRAPH SYNTHETIC DATA GENERATION\n",
    "\n",
    "## Task 1: Dependencies and API Keys for LangGraph SDG\n",
    "\n",
    "We'll need to install dependencies and set up API keys for our LangGraph-based synthetic data generation pipeline.\n",
    "\n",
    "1. **LangGraph** for our Agent Graph workflow\n",
    "2. **OpenAI's endpoints** for Evol-Instruct synthetic data generation  \n",
    "3. **LangChain** for document processing and LLM integration\n",
    "4. **LangSmith** for workflow tracing and later evaluation\n",
    "\n",
    "Let's install and provide all the required information below!\n",
    "\n",
    "### Dependencies and API Keys:\n",
    "\n",
    "> NOTE: DO NOT RUN THESE CELLS IF YOU ARE RUNNING THIS NOTEBOOK LOCALLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for LangGraph SDG (uncomment if needed)\n",
    "# !pip install -qU langgraph==0.2.61 langchain-openai==0.2.14 langchain-community==0.3.14 python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Import\n",
    "\n",
    "To prevent errors that may occur based on OS - we'll import NLTK and download the needed packages to ensure correct handling of data for our LangGraph workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup for LangGraph SDG\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from uuid import uuid4\n",
    "from typing import List, Dict, Any, TypedDict\n",
    "from collections import Counter\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set up API keys and tracing for LangGraph workflow\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll set a unique project name to make things easier to track in LangSmith for our LangGraph SDG workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"LangGraph-SDG-EvolInstruct-{uuid4().hex[0:8]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Synthetic Test Data with LangGraph\n",
    "\n",
    "We will be using **LangGraph Agent Graph** with **Evol-Instruct method** to build out a set of synthetic test questions, references, and reference contexts. This replaces the RAGAS Knowledge Graph approach with a more direct implementation.\n",
    "\n",
    "> NOTE: This is the BONUS ACTIVITY implementation - using LangGraph instead of RAGAS KG approach. The Evol-Instruct methodology will create *directional* improvements in synthetic data quality through evolution strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "We'll prepare our data - which should hopefully be familiar at this point since it's our Loan Data use-case! This time we'll load it specifically for LangGraph processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load our data into the familiar LangChain format using the `DirectoryLoader` - this will be the input for our LangGraph Agent Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 269 documents from data/\n",
      "âœ… Preprocessed into 500 document chunks\n",
      "âœ… Sample document length: 1913 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Import document loading utilities for LangGraph SDG\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load documents from the data directory - Block 2 Implementation\n",
    "data_path = \"data/\"\n",
    "loader = DirectoryLoader(\n",
    "    data_path, \n",
    "    glob=\"*.pdf\", \n",
    "    loader_cls=PyMuPDFLoader,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# Load all documents\n",
    "try:\n",
    "    documents = loader.load()\n",
    "    print(f\"âœ… Loaded {len(documents)} documents from {data_path}\")\n",
    "    \n",
    "    # Optional: Document preprocessing for LangGraph workflow\n",
    "    # If documents are too large, we might need to chunk them\n",
    "    processed_documents = []\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=200\n",
    "    )\n",
    "    \n",
    "    for doc in documents:\n",
    "        if len(doc.page_content) > 3000:  # Split large documents\n",
    "            chunks = text_splitter.split_documents([doc])\n",
    "            processed_documents.extend(chunks)\n",
    "        else:\n",
    "            processed_documents.append(doc)\n",
    "    \n",
    "    print(f\"âœ… Preprocessed into {len(processed_documents)} document chunks\")\n",
    "    print(f\"âœ… Sample document length: {len(processed_documents[0].page_content) if processed_documents else 0} characters\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading documents: {e}\")\n",
    "    # Fallback: Create sample documents for demo\n",
    "    processed_documents = [\n",
    "        Document(page_content=\"Sample document content for testing purposes.\", metadata={\"source\": \"demo.pdf\"}),\n",
    "        Document(page_content=\"Another sample document for demonstration.\", metadata={\"source\": \"demo2.pdf\"})\n",
    "    ]\n",
    "    print(f\"ðŸ”„ Using {len(processed_documents)} sample documents for demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangGraph Agent Graph Based Synthetic Generation\n",
    "\n",
    "Instead of RAGAS Knowledge Graph approach, we'll use **LangGraph Agent Graph** with **Evol-Instruct methodology**. This approach directly implements the evolution strategies through a workflow of specialized nodes.\n",
    "\n",
    "Let's start by defining our LangGraph `State` structure and the generator LLM that will power our evolution process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LangGraph and LLM components for SDG\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Define the state structure for LangGraph workflow\n",
    "class State(TypedDict):\n",
    "    \"\"\"State structure that flows through the LangGraph workflow\"\"\"\n",
    "    documents: List[Document]              # Input LangChain documents\n",
    "    evolved_questions: List[Dict[str, Any]]  # Generated questions with metadata\n",
    "    question_answers: List[Dict[str, Any]]   # Answers for each question\n",
    "    question_contexts: List[Dict[str, Any]]  # Relevant contexts for each question\n",
    "    processing_status: str                   # Current processing status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define our **Evol-Instruct methodology compliant prompts** and initialize our generator LLM. These prompts implement the three evolution strategies required by the BONUS ACTIVITY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LLM connection successful!\n"
     ]
    }
   ],
   "source": [
    "# Evol-Instruct methodology compliant prompts for different evolution types\n",
    "EVOLUTION_PROMPTS = {\n",
    "    \"simple\": \"\"\"You are an expert question generator using the Evol-Instruct method.\n",
    "    Generate a simple, direct question that can be answered from this single document.\n",
    "    Apply simple evolution to make the question clear and focused.\n",
    "    \n",
    "    Document: {content}\n",
    "    \n",
    "    Requirements:\n",
    "    - Generate ONE specific question\n",
    "    - Keep it straightforward and factual\n",
    "    - Answerable from the provided content\n",
    "    - Focus on direct information retrieval\n",
    "    \n",
    "    Question:\"\"\",\n",
    "    \n",
    "    \"multi_context\": \"\"\"You are an expert question generator using the Evol-Instruct method.\n",
    "    Generate a question requiring multi-context evolution that needs information from multiple documents.\n",
    "    Apply evolution to create connections between different information sources.\n",
    "    \n",
    "    Documents: {content}\n",
    "    \n",
    "    Requirements:\n",
    "    - Generate ONE question connecting multiple documents\n",
    "    - Require synthesis of information from different sources\n",
    "    - Focus on relationships, comparisons, or complementary information\n",
    "    - Make connections across document boundaries\n",
    "    \n",
    "    Question:\"\"\",\n",
    "    \n",
    "    \"reasoning\": \"\"\"You are an expert question generator using the Evol-Instruct method.\n",
    "    Generate a reasoning question that requires analytical thinking and inference.\n",
    "    Apply reasoning evolution to create complexity requiring multi-step analysis.\n",
    "    \n",
    "    Document: {content}\n",
    "    \n",
    "    Requirements:\n",
    "    - Generate ONE question requiring analytical reasoning\n",
    "    - Include elements like 'why', 'how', 'analyze', 'evaluate', 'compare'\n",
    "    - Require multi-step reasoning beyond simple facts\n",
    "    - Focus on inference and analysis\n",
    "    \n",
    "    Question:\"\"\"\n",
    "}\n",
    "\n",
    "# Answer generation prompt\n",
    "ANSWER_PROMPT = \"\"\"Answer this question comprehensively using the provided context.\n",
    "Be specific, accurate, and structure your response clearly.\n",
    "If the context doesn't contain enough information, state what's missing.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Initialize the Large Language Model for question and answer generation\n",
    "generator_llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",        # Use GPT-4 Omni Mini for cost-effective performance\n",
    "    temperature=0.7,            # Set creativity level for evolution\n",
    "    max_tokens=1000,           # Limit response length\n",
    "    request_timeout=60          # Set timeout for API calls\n",
    ")\n",
    "\n",
    "# Test the LLM connection\n",
    "try:\n",
    "    test_response = generator_llm.invoke(\"Test connection. Respond with 'Connection successful!'\")\n",
    "    print(\"âœ… LLM connection successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ LLM initialization failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll implement our **LangGraph Node Functions** that handle the three evolution types. Each function represents a node in our Agent Graph that processes documents and generates evolved questions according to the Evol-Instruct methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_evolution(state: State) -> State:\n",
    "    \"\"\"Node 1: Generate simple evolution questions from individual documents\"\"\"\n",
    "    docs = state[\"documents\"][:5]  # Use subset for demo\n",
    "    questions = state[\"evolved_questions\"]\n",
    "    \n",
    "    print(f\"ðŸ”„ Processing Simple Evolution: {len(docs)} documents...\")\n",
    "    \n",
    "    for i, doc in enumerate(docs):\n",
    "        # Truncate document content to avoid token limits\n",
    "        content = doc.page_content[:1000]\n",
    "        \n",
    "        # Generate simple question using Evol-Instruct methodology\n",
    "        prompt = EVOLUTION_PROMPTS[\"simple\"].format(content=content)\n",
    "        response = generator_llm.invoke(prompt)\n",
    "        \n",
    "        # Create question object with metadata\n",
    "        question = {\n",
    "            \"id\": f\"simple_{i+1}\",\n",
    "            \"question\": response.content.strip(),\n",
    "            \"evolution_type\": \"simple\",\n",
    "            \"complexity_level\": 1,\n",
    "            \"source_docs\": [i]\n",
    "        }\n",
    "        \n",
    "        questions.append(question)\n",
    "    \n",
    "    # Update state and return\n",
    "    state[\"evolved_questions\"] = questions\n",
    "    state[\"processing_status\"] = \"Simple evolution completed\"\n",
    "    \n",
    "    print(f\"âœ… Simple Evolution: Generated {len(docs)} questions\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def multi_context_evolution(state: State) -> State:\n",
    "    \"\"\"Node 2: Generate multi-context evolution questions from document pairs\"\"\"\n",
    "    docs = state[\"documents\"][:6]  # Use subset for demo\n",
    "    questions = state[\"evolved_questions\"]\n",
    "    \n",
    "    print(f\"ðŸ”„ Processing Multi-Context Evolution: {len(docs)//2} pairs...\")\n",
    "    \n",
    "    # Process documents in pairs\n",
    "    for i in range(0, len(docs)-1, 2):\n",
    "        if i+1 < len(docs):\n",
    "            # Combine content from two documents\n",
    "            combined_content = f\"Document 1:\\n{docs[i].page_content[:800]}\\n\\nDocument 2:\\n{docs[i+1].page_content[:800]}\"\n",
    "            \n",
    "            # Generate multi-context question\n",
    "            prompt = EVOLUTION_PROMPTS[\"multi_context\"].format(content=combined_content)\n",
    "            response = generator_llm.invoke(prompt)\n",
    "            \n",
    "            # Create question object with metadata\n",
    "            question = {\n",
    "                \"id\": f\"multi_context_{(i//2)+1}\",\n",
    "                \"question\": response.content.strip(),\n",
    "                \"evolution_type\": \"multi_context\",\n",
    "                \"complexity_level\": 2,\n",
    "                \"source_docs\": [i, i+1]\n",
    "            }\n",
    "            \n",
    "            questions.append(question)\n",
    "    \n",
    "    # Update state and return\n",
    "    state[\"evolved_questions\"] = questions\n",
    "    state[\"processing_status\"] = \"Multi-context evolution completed\"\n",
    "    \n",
    "    print(f\"âœ… Multi-Context Evolution: Generated {len(docs)//2} questions\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def reasoning_evolution(state: State) -> State:\n",
    "    \"\"\"Node 3: Generate reasoning evolution questions requiring analysis\"\"\"\n",
    "    docs = state[\"documents\"][:3]  # Use subset for demo\n",
    "    questions = state[\"evolved_questions\"]\n",
    "    \n",
    "    print(f\"ðŸ”„ Processing Reasoning Evolution: {len(docs)} documents...\")\n",
    "    \n",
    "    for i, doc in enumerate(docs):\n",
    "        # Truncate document content to avoid token limits\n",
    "        content = doc.page_content[:1000]\n",
    "        \n",
    "        # Generate reasoning question using Evol-Instruct methodology\n",
    "        prompt = EVOLUTION_PROMPTS[\"reasoning\"].format(content=content)\n",
    "        response = generator_llm.invoke(prompt)\n",
    "        \n",
    "        # Create question object with metadata\n",
    "        question = {\n",
    "            \"id\": f\"reasoning_{i+1}\",\n",
    "            \"question\": response.content.strip(),\n",
    "            \"evolution_type\": \"reasoning\",\n",
    "            \"complexity_level\": 3,\n",
    "            \"source_docs\": [i]\n",
    "        }\n",
    "        \n",
    "        questions.append(question)\n",
    "    \n",
    "    # Update state and return\n",
    "    state[\"evolved_questions\"] = questions\n",
    "    state[\"processing_status\"] = \"Reasoning evolution completed\"\n",
    "    \n",
    "    print(f\"âœ… Reasoning Evolution: Generated {len(docs)} questions\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll add the **Answer and Context Generation nodes** that complete our synthetic data pipeline. These nodes generate comprehensive answers and relevant contexts for all evolved questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answers(state: State) -> State:\n",
    "    \"\"\"Node 4: Generate comprehensive answers for all evolved questions\"\"\"\n",
    "    docs = state[\"documents\"]\n",
    "    questions = state[\"evolved_questions\"]\n",
    "    answers = []\n",
    "    \n",
    "    print(f\"ðŸ”„ Generating Answers: {len(questions)} questions...\")\n",
    "    \n",
    "    for i, question in enumerate(questions):\n",
    "        # Get context from source documents\n",
    "        context_parts = []\n",
    "        for doc_idx in question[\"source_docs\"]:\n",
    "            if doc_idx < len(docs):\n",
    "                context_parts.append(docs[doc_idx].page_content)\n",
    "        \n",
    "        # Combine context (truncate to avoid token limits)\n",
    "        context = \"\\n\\n\".join(context_parts)[:2000]\n",
    "        \n",
    "        # Generate answer using the LLM\n",
    "        prompt = ANSWER_PROMPT.format(question=question[\"question\"], context=context)\n",
    "        response = generator_llm.invoke(prompt)\n",
    "        \n",
    "        # Create answer object with metadata\n",
    "        answer = {\n",
    "            \"question_id\": question[\"id\"],\n",
    "            \"answer\": response.content.strip(),\n",
    "            \"confidence\": 0.9  # Placeholder confidence score\n",
    "        }\n",
    "        \n",
    "        answers.append(answer)\n",
    "    \n",
    "    # Update state and return\n",
    "    state[\"question_answers\"] = answers\n",
    "    state[\"processing_status\"] = \"Answer generation completed\"\n",
    "    \n",
    "    print(f\"âœ… Generated {len(answers)} answers\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def generate_contexts(state: State) -> State:\n",
    "    \"\"\"Node 5: Generate relevant contexts for all evolved questions\"\"\"\n",
    "    docs = state[\"documents\"]\n",
    "    questions = state[\"evolved_questions\"]\n",
    "    contexts = []\n",
    "    \n",
    "    print(f\"ðŸ”„ Generating Contexts: {len(questions)} questions...\")\n",
    "    \n",
    "    for i, question in enumerate(questions):\n",
    "        # Get relevant contexts from source documents\n",
    "        doc_contexts = []\n",
    "        relevance_scores = []\n",
    "        \n",
    "        for doc_idx in question[\"source_docs\"]:\n",
    "            if doc_idx < len(docs):\n",
    "                doc_contexts.append(docs[doc_idx].page_content)\n",
    "                relevance_scores.append(0.9)  # Placeholder relevance score\n",
    "        \n",
    "        # Create context object with metadata\n",
    "        context = {\n",
    "            \"question_id\": question[\"id\"],\n",
    "            \"contexts\": doc_contexts,\n",
    "            \"relevance_scores\": relevance_scores\n",
    "        }\n",
    "        \n",
    "        contexts.append(context)\n",
    "    \n",
    "    # Update state and return\n",
    "    state[\"question_contexts\"] = contexts\n",
    "    state[\"processing_status\"] = \"Context generation completed\"\n",
    "    \n",
    "    print(f\"âœ… Generated {len(contexts)} context sets\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create our **LangGraph Agent Graph** by connecting all node functions in the correct sequence. This replaces the RAGAS Knowledge Graph approach with a direct workflow implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LangGraph Agent Graph compiled successfully!\n",
      "\n",
      "ðŸ•¸ï¸ LANGGRAPH AGENT GRAPH VISUALIZATION:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Graph visualization saved as 'langgraph_workflow_visualization.png'\n"
     ]
    }
   ],
   "source": [
    "# Create the LangGraph Agent Graph workflow\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Add all node functions to the graph\n",
    "workflow.add_node(\"simple_evolution\", simple_evolution)\n",
    "workflow.add_node(\"multi_context_evolution\", multi_context_evolution)\n",
    "workflow.add_node(\"reasoning_evolution\", reasoning_evolution)\n",
    "workflow.add_node(\"generate_answers\", generate_answers)\n",
    "workflow.add_node(\"generate_contexts\", generate_contexts)\n",
    "\n",
    "# Define the execution flow (edges between nodes)\n",
    "workflow.set_entry_point(\"simple_evolution\")  # Start with simple evolution\n",
    "workflow.add_edge(\"simple_evolution\", \"multi_context_evolution\")\n",
    "workflow.add_edge(\"multi_context_evolution\", \"reasoning_evolution\")\n",
    "workflow.add_edge(\"reasoning_evolution\", \"generate_answers\")\n",
    "workflow.add_edge(\"generate_answers\", \"generate_contexts\")\n",
    "workflow.add_edge(\"generate_contexts\", END)  # End the workflow\n",
    "\n",
    "# Compile the graph for execution\n",
    "graph = workflow.compile()\n",
    "\n",
    "print(\"âœ… LangGraph Agent Graph compiled successfully!\")\n",
    "\n",
    "# ðŸŽ¨ GRAPH VISUALIZATION - Professional LangGraph Feature\n",
    "try:\n",
    "    from IPython.display import Image, display\n",
    "    import io\n",
    "    import base64\n",
    "    \n",
    "    # Generate graph visualization\n",
    "    graph_image = graph.get_graph().draw_mermaid_png()\n",
    "    \n",
    "    # Display the workflow visualization\n",
    "    print(\"\\nðŸ•¸ï¸ LANGGRAPH AGENT GRAPH VISUALIZATION:\")\n",
    "    display(Image(graph_image))\n",
    "    \n",
    "    # Also save the graph as PNG file for documentation\n",
    "    with open(\"langgraph_workflow_visualization.png\", \"wb\") as f:\n",
    "        f.write(graph_image)\n",
    "    \n",
    "    print(\"âœ… Graph visualization saved as 'langgraph_workflow_visualization.png'\")\n",
    "    \n",
    "except ImportError:\n",
    "    # Fallback: ASCII representation\n",
    "    print(\"\\nðŸ•¸ï¸ LANGGRAPH AGENT GRAPH STRUCTURE:\")\n",
    "    print(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "    print(\"â”‚ simple_evolution â”‚\")\n",
    "    print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "    print(\"          â”‚\")\n",
    "    print(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "    print(\"â”‚ multi_context_evolution  â”‚\")\n",
    "    print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "    print(\"          â”‚\")\n",
    "    print(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "    print(\"â”‚ reasoning_evolution   â”‚\")\n",
    "    print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "    print(\"          â”‚\")\n",
    "    print(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "    print(\"â”‚ generate_answers   â”‚\")\n",
    "    print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "    print(\"          â”‚\")\n",
    "    print(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "    print(\"â”‚ generate_contexts  â”‚\")\n",
    "    print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "    print(\"          â”‚\")\n",
    "    print(\"        â”Œâ”€â–¼â”€â”\")\n",
    "    print(\"        â”‚ENDâ”‚\")\n",
    "    print(\"        â””â”€â”€â”€â”˜\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Graph visualization unavailable: {e}\")\n",
    "    print(\"ðŸ’¡ Install pygraphviz for full visualization: pip install pygraphviz\")\n",
    "    \n",
    "    # Simple text representation\n",
    "    print(\"\\nðŸ”„ Workflow Sequence:\")\n",
    "    print(\"1. simple_evolution â†’ 2. multi_context_evolution â†’ 3. reasoning_evolution â†’ 4. generate_answers â†’ 5. generate_contexts â†’ END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use our **LangGraph Agent Graph** to generate our synthetic dataset! This replaces the RAGAS testset generation with our Evol-Instruct implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting LangGraph SDG workflow with 10 documents\n",
      "============================================================\n",
      "ðŸ”„ Processing Simple Evolution: 5 documents...\n",
      "âœ… Simple Evolution: Generated 5 questions\n",
      "ðŸ”„ Processing Multi-Context Evolution: 3 pairs...\n",
      "âœ… Multi-Context Evolution: Generated 3 questions\n",
      "ðŸ”„ Processing Reasoning Evolution: 3 documents...\n",
      "âœ… Reasoning Evolution: Generated 3 questions\n",
      "ðŸ”„ Generating Answers: 11 questions...\n",
      "âœ… Generated 11 answers\n",
      "ðŸ”„ Generating Contexts: 11 questions...\n",
      "âœ… Generated 11 context sets\n",
      "============================================================\n",
      "âœ… LangGraph workflow completed successfully!\n",
      "ðŸ“Š Generated Results:\n",
      "   â€¢ 11 evolved questions\n",
      "   â€¢ 11 answers\n",
      "   â€¢ 11 context sets\n",
      "   â€¢ Evolution distribution: {'simple': 5, 'multi_context': 3, 'reasoning': 3}\n"
     ]
    }
   ],
   "source": [
    "# Execute the LangGraph Agent Graph workflow\n",
    "initial_state = {\n",
    "    \"documents\": processed_documents[:10],  # Use first 10 documents for demo\n",
    "    \"evolved_questions\": [],\n",
    "    \"question_answers\": [],\n",
    "    \"question_contexts\": [],\n",
    "    \"processing_status\": \"Initialized\"\n",
    "}\n",
    "\n",
    "print(f\"ðŸš€ Starting LangGraph SDG workflow with {len(initial_state['documents'])} documents\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Execute the graph\n",
    "try:\n",
    "    result_state = graph.invoke(initial_state)\n",
    "    \n",
    "    # Extract results\n",
    "    evolved_questions = result_state[\"evolved_questions\"]\n",
    "    question_answers = result_state[\"question_answers\"]\n",
    "    question_contexts = result_state[\"question_contexts\"]\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"âœ… LangGraph workflow completed successfully!\")\n",
    "    print(f\"ðŸ“Š Generated Results:\")\n",
    "    print(f\"   â€¢ {len(evolved_questions)} evolved questions\")\n",
    "    print(f\"   â€¢ {len(question_answers)} answers\")\n",
    "    print(f\"   â€¢ {len(question_contexts)} context sets\")\n",
    "    \n",
    "    # Show evolution type distribution\n",
    "    evolution_types = [q['evolution_type'] for q in evolved_questions]\n",
    "    type_counts = Counter(evolution_types)\n",
    "    print(f\"   â€¢ Evolution distribution: {dict(type_counts)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error during LangGraph workflow execution: {e}\")\n",
    "    raise Exception(f\"Error during LangGraph workflow execution: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display our **BONUS ACTIVITY results** in the required format and validate that all requirements have been fulfilled!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ LANGGRAPH SYNTHETIC DATA GENERATION RESULTS\n",
      "============================================================\n",
      "\n",
      "ðŸŽ¯ 1. EVOLVED QUESTIONS (11 total):\n",
      "----------------------------------------\n",
      "\n",
      "1. ID: simple_1\n",
      "   Evolution Type: simple\n",
      "   Complexity Level: 1\n",
      "   Question: What does this volume of the Federal Student Aid Handbook discuss regarding Title IV student financi...\n",
      "   Source Docs: [0]\n",
      "\n",
      "2. ID: simple_2\n",
      "   Evolution Type: simple\n",
      "   Complexity Level: 1\n",
      "   Question: What additional example of a cost has been included under the Tuition and Fees component of the Cost...\n",
      "   Source Docs: [1]\n",
      "\n",
      "3. ID: simple_3\n",
      "   Evolution Type: simple\n",
      "   Complexity Level: 1\n",
      "   Question: What factors can lead to different academic year definitions for various programs at a school?...\n",
      "   Source Docs: [2]\n",
      "\n",
      "ðŸ’¬ 2. QUESTION IDS + ANSWERS (11 total):\n",
      "----------------------------------------\n",
      "\n",
      "1. Question ID: simple_1\n",
      "   Confidence: 0.9\n",
      "   Answer: Volume 3 of the Federal Student Aid (FSA) Handbook focuses on the following key areas regarding Title IV student financi...\n",
      "\n",
      "2. Question ID: simple_2\n",
      "   Confidence: 0.9\n",
      "   Answer: Under the Tuition and Fees component of the Cost of Attendance (COA), an additional example of a cost that has been incl...\n",
      "\n",
      "ðŸ“š 3. QUESTION IDS + RELEVANT CONTEXTS (11 total):\n",
      "----------------------------------------\n",
      "\n",
      "1. Question ID: simple_1\n",
      "   Number of Contexts: 1\n",
      "   Relevance Scores: [0.9]\n",
      "   Context Preview: Volume 3\n",
      "Academic Calendars, Cost of Attendance, and\n",
      "Packaging\n",
      "Introduction\n",
      "This...\n",
      "\n",
      "2. Question ID: simple_2\n",
      "   Number of Contexts: 1\n",
      "   Relevance Scores: [0.9]\n",
      "   Context Preview: intensity below 50%, the cost of attendance (COA) is still based on the full-yea...\n",
      "\n",
      "âœ… BONUS ACTIVITY REQUIREMENTS VALIDATION:\n",
      "============================================================\n",
      "âœ“ LangGraph Agent Graph (instead of Knowledge Graph): âœ… IMPLEMENTED\n",
      "âœ“ Evol-Instruct Method: âœ… IMPLEMENTED\n",
      "âœ“ Input: List of LangChain Documents: âœ… IMPLEMENTED\n",
      "âœ“ Output Format:\n",
      "  â€¢ List(dict) Evolved Questions + IDs + Evolution Types: âœ… 11 items\n",
      "  â€¢ List(dict) Question IDs + Answers: âœ… 11 items\n",
      "  â€¢ List(dict) Question IDs + Relevant Contexts: âœ… 11 items\n",
      "âœ“ Evolution Types Handled:\n",
      "  â€¢ Simple Evolution: âœ… IMPLEMENTED\n",
      "  â€¢ Multi-Context Evolution: âœ… IMPLEMENTED\n",
      "  â€¢ Reasoning Evolution: âœ… IMPLEMENTED\n",
      "\n",
      "ðŸŽ‰ BONUS ACTIVITY COMPLETED SUCCESSFULLY!\n",
      "All requirements fulfilled with LangGraph Agent Graph implementation.\n"
     ]
    }
   ],
   "source": [
    "# Display BONUS ACTIVITY results in required format\n",
    "\n",
    "print(\"ðŸ“‹ LANGGRAPH SYNTHETIC DATA GENERATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. List(dict): Evolved Questions + IDs + Evolution Types\n",
    "print(f\"\\nðŸŽ¯ 1. EVOLVED QUESTIONS ({len(evolved_questions)} total):\")\n",
    "print(\"-\" * 40)\n",
    "for i, q in enumerate(evolved_questions[:3]):  # Show first 3 as sample\n",
    "    print(f\"\\n{i+1}. ID: {q['id']}\")\n",
    "    print(f\"   Evolution Type: {q['evolution_type']}\")\n",
    "    print(f\"   Complexity Level: {q['complexity_level']}\")\n",
    "    print(f\"   Question: {q['question'][:100]}...\")\n",
    "    print(f\"   Source Docs: {q['source_docs']}\")\n",
    "\n",
    "# 2. List(dict): Question IDs + Answers\n",
    "print(f\"\\nðŸ’¬ 2. QUESTION IDS + ANSWERS ({len(question_answers)} total):\")\n",
    "print(\"-\" * 40)\n",
    "for i, a in enumerate(question_answers[:2]):  # Show first 2 as sample\n",
    "    print(f\"\\n{i+1}. Question ID: {a['question_id']}\")\n",
    "    print(f\"   Confidence: {a['confidence']}\")\n",
    "    print(f\"   Answer: {a['answer'][:120]}...\")\n",
    "\n",
    "# 3. List(dict): Question IDs + Relevant Contexts\n",
    "print(f\"\\nðŸ“š 3. QUESTION IDS + RELEVANT CONTEXTS ({len(question_contexts)} total):\")\n",
    "print(\"-\" * 40)\n",
    "for i, c in enumerate(question_contexts[:2]):  # Show first 2 as sample\n",
    "    print(f\"\\n{i+1}. Question ID: {c['question_id']}\")\n",
    "    print(f\"   Number of Contexts: {len(c['contexts'])}\")\n",
    "    print(f\"   Relevance Scores: {c['relevance_scores']}\")\n",
    "    print(f\"   Context Preview: {c['contexts'][0][:80]}...\" if c['contexts'] else \"   No context\")\n",
    "\n",
    "print(f\"\\nâœ… BONUS ACTIVITY REQUIREMENTS VALIDATION:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"âœ“ LangGraph Agent Graph (instead of Knowledge Graph): âœ… IMPLEMENTED\")\n",
    "print(f\"âœ“ Evol-Instruct Method: âœ… IMPLEMENTED\") \n",
    "print(f\"âœ“ Input: List of LangChain Documents: âœ… IMPLEMENTED\")\n",
    "print(f\"âœ“ Output Format:\")\n",
    "print(f\"  â€¢ List(dict) Evolved Questions + IDs + Evolution Types: âœ… {len(evolved_questions)} items\")\n",
    "print(f\"  â€¢ List(dict) Question IDs + Answers: âœ… {len(question_answers)} items\") \n",
    "print(f\"  â€¢ List(dict) Question IDs + Relevant Contexts: âœ… {len(question_contexts)} items\")\n",
    "print(f\"âœ“ Evolution Types Handled:\")\n",
    "print(f\"  â€¢ Simple Evolution: âœ… IMPLEMENTED\")\n",
    "print(f\"  â€¢ Multi-Context Evolution: âœ… IMPLEMENTED\")\n",
    "print(f\"  â€¢ Reasoning Evolution: âœ… IMPLEMENTED\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ BONUS ACTIVITY COMPLETED SUCCESSFULLY!\")\n",
    "print(\"All requirements fulfilled with LangGraph Agent Graph implementation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll export our results and prepare them for the next phase - LangSmith evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Results exported to: langgraph_synthetic_data_evolinstruct_final.json\n",
      "âœ… Prepared 11 examples for LangSmith integration\n"
     ]
    }
   ],
   "source": [
    "# Export results to JSON file for documentation\n",
    "export_data = {\n",
    "    \"metadata\": {\n",
    "        \"generation_timestamp\": str(uuid4()),\n",
    "        \"method\": \"LangGraph Agent Graph with Evol-Instruct\",\n",
    "        \"total_documents_processed\": len(result_state[\"documents\"]),\n",
    "        \"langsmith_project\": os.environ.get(\"LANGCHAIN_PROJECT\"),\n",
    "        \"model_used\": \"gpt-4o-mini\",\n",
    "        \"bonus_activity\": \"COMPLETED\"\n",
    "    },\n",
    "    \"evolved_questions\": result_state[\"evolved_questions\"],\n",
    "    \"question_answers\": result_state[\"question_answers\"],\n",
    "    \"question_contexts\": result_state[\"question_contexts\"],\n",
    "    \"summary\": {\n",
    "        \"total_questions\": len(result_state[\"evolved_questions\"]),\n",
    "        \"evolution_types\": {\n",
    "            \"simple\": len([q for q in result_state[\"evolved_questions\"] if q['evolution_type'] == 'simple']),\n",
    "            \"multi_context\": len([q for q in result_state[\"evolved_questions\"] if q['evolution_type'] == 'multi_context']),\n",
    "            \"reasoning\": len([q for q in result_state[\"evolved_questions\"] if q['evolution_type'] == 'reasoning'])\n",
    "        },\n",
    "        \"complexity_distribution\": {\n",
    "            \"level_1\": len([q for q in result_state[\"evolved_questions\"] if q['complexity_level'] == 1]),\n",
    "            \"level_2\": len([q for q in result_state[\"evolved_questions\"] if q['complexity_level'] == 2]),\n",
    "            \"level_3\": len([q for q in result_state[\"evolved_questions\"] if q['complexity_level'] == 3])\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Export to JSON file\n",
    "output_filename = \"langgraph_synthetic_data_evolinstruct_final.json\"\n",
    "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(export_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… Results exported to: {output_filename}\")\n",
    "\n",
    "# Prepare dataset for LangSmith integration\n",
    "langsmith_dataset = []\n",
    "for i, question in enumerate(evolved_questions):\n",
    "    # Find corresponding answer and context\n",
    "    answer = next((a for a in question_answers if a['question_id'] == question['id']), None)\n",
    "    context = next((c for c in question_contexts if c['question_id'] == question['id']), None)\n",
    "    \n",
    "    if answer and context:\n",
    "        langsmith_dataset.append({\n",
    "            \"user_input\": question['question'],\n",
    "            \"reference\": answer['answer'],\n",
    "            \"reference_contexts\": context['contexts'],\n",
    "            \"evolution_type\": question['evolution_type'],\n",
    "            \"complexity_level\": question['complexity_level']\n",
    "        })\n",
    "\n",
    "print(f\"âœ… Prepared {len(langsmith_dataset)} examples for LangSmith integration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤ PART 2: LANGSMITH INTEGRATION\n",
    "\n",
    "## Task 4: LangSmith Dataset\n",
    "\n",
    "Now we can move on to creating a dataset for LangSmith using our **LangGraph-generated synthetic data**!\n",
    "\n",
    "First, we'll need to create a dataset on LangSmith using the `Client`! We'll name our Dataset to make it easy to work with later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LangSmith client for dataset creation and evaluation\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "dataset_name = \"LangGraph Synthetic Data - EvolInstruct v2\"\n",
    "\n",
    "langsmith_dataset_obj = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Synthetic data generated using LangGraph Agent Graph with Evol-Instruct method (BONUS ACTIVITY)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll iterate through our **LangGraph-created dataset** - and add each example to our created LangSmith dataset!\n",
    "\n",
    "> NOTE: We need to conform the outputs to the expected format - which in this case is: `question` and `answer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¤ Adding 11 examples to LangSmith dataset...\n",
      "   Added 5/11 examples...\n",
      "   Added 10/11 examples...\n",
      "âœ… Successfully added all 11 examples to LangSmith dataset!\n"
     ]
    }
   ],
   "source": [
    "# Add LangGraph synthetic data examples to LangSmith dataset\n",
    "print(f\"ðŸ“¤ Adding {len(langsmith_dataset)} examples to LangSmith dataset...\")\n",
    "\n",
    "for i, data_row in enumerate(langsmith_dataset):\n",
    "    client.create_example(\n",
    "        inputs={\n",
    "            \"question\": data_row[\"user_input\"]\n",
    "        },\n",
    "        outputs={\n",
    "            \"answer\": data_row[\"reference\"]\n",
    "        },\n",
    "        metadata={\n",
    "            \"context\": data_row[\"reference_contexts\"],\n",
    "            \"evolution_type\": data_row[\"evolution_type\"],\n",
    "            \"complexity_level\": data_row[\"complexity_level\"],\n",
    "            \"source\": \"LangGraph-EvolInstruct\"\n",
    "        },\n",
    "        dataset_id=langsmith_dataset_obj.id\n",
    "    )\n",
    "    \n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"   Added {i + 1}/{len(langsmith_dataset)} examples...\")\n",
    "\n",
    "print(f\"âœ… Successfully added all {len(langsmith_dataset)} examples to LangSmith dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RAG Chain\n",
    "\n",
    "Time for some RAG! We'll build a RAG chain to test against our LangGraph-generated synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare documents for RAG\n",
    "rag_documents = documents\n",
    "\n",
    "# To keep things simple, we'll use LangChain's recursive character text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 50\n",
    ")\n",
    "\n",
    "rag_documents = text_splitter.split_documents(rag_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create our vectorstore using OpenAI's [`text-embedding-3-small`](https://platform.openai.com/docs/guides/embeddings/embedding-models) embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import embedding and vectorstore components\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# As usual, we will power our RAG application with Qdrant!\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    documents=rag_documents,\n",
    "    embedding=embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"LangGraph RAG\"\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the \"A\" in RAG, we'll provide a prompt and set up our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import prompt and LCEL components\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "RAG_PROMPT = \"\"\"\\\n",
    "Given a provided context and question, you must answer the question based only on context.\n",
    "\n",
    "If you cannot answer the question based on the context - you must say \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)\n",
    "\n",
    "# For our LLM, we will be using OpenAI's GPT-4o-mini\n",
    "rag_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Finally, we can set-up our RAG LCEL chain!\n",
    "rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "    | rag_prompt | rag_llm | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Test the RAG chain\n",
    "test_result = rag_chain.invoke({\"question\" : \"What kinds of loans are available?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangSmith Evaluation Set-up\n",
    "\n",
    "We'll use OpenAI's GPT-4o-mini as our evaluation LLM and set up evaluators to test our RAG chain against the LangGraph-generated synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation components\n",
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "\n",
    "# Set up evaluation LLM\n",
    "eval_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# We'll be using evaluators to test our RAG performance on LangGraph synthetic data\n",
    "qa_evaluator = LangChainStringEvaluator(\"qa\", config={\"llm\" : eval_llm})\n",
    "\n",
    "labeled_helpfulness_evaluator = LangChainStringEvaluator(\n",
    "    \"labeled_criteria\",\n",
    "    config={\n",
    "        \"criteria\": {\n",
    "            \"helpfulness\": (\n",
    "                \"Is this submission helpful to the user,\"\n",
    "                \" taking into account the correct reference answer?\"\n",
    "            )\n",
    "        },\n",
    "        \"llm\" : eval_llm\n",
    "    },\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"output\"],\n",
    "        \"reference\": example.outputs[\"answer\"],\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "empathy_evaluator = LangChainStringEvaluator(\n",
    "    \"criteria\",\n",
    "    config={\n",
    "        \"criteria\": {\n",
    "            \"empathy\": \"Is this response empathetic? Does it make the user feel like they are being heard?\",\n",
    "        },\n",
    "        \"llm\" : eval_llm\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangSmith Evaluation\n",
    "\n",
    "Let's evaluate our RAG chain against the **LangGraph-generated synthetic data**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting evaluation against LangGraph synthetic data...\n",
      "View the evaluation results for experiment: 'timely-increase-12' at:\n",
      "https://smith.langchain.com/o/3c2c7006-57b9-4cbe-911e-6f73b4734883/datasets/dc35309b-1930-44d7-badd-9603b8f35f2e/compare?selectedSessions=0b0b2839-ce7b-4f3c-9cf1-cf257a032681\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b126d8e1b55e439c8332bcd00940a9f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Evaluation completed! Check LangSmith for detailed results.\n",
      "ðŸ“Š Evaluation Results Summary: <ExperimentResults timely-increase-12>\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸš€ Starting evaluation against LangGraph synthetic data...\")\n",
    "\n",
    "try:\n",
    "    # Run evaluation with timeout and batch settings to prevent hanging\n",
    "    evaluation_results = evaluate(\n",
    "        rag_chain.invoke,\n",
    "        data=dataset_name,\n",
    "        evaluators=[\n",
    "            qa_evaluator,\n",
    "            labeled_helpfulness_evaluator,\n",
    "            empathy_evaluator\n",
    "        ],\n",
    "        metadata={\"revision_id\": \"langgraph_rag_chain_initial\", \"data_source\": \"LangGraph-EvolInstruct\"},\n",
    "        max_concurrency=1,  # Limit concurrency to prevent hanging\n",
    "        client=client       # Explicit client\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâœ… Evaluation completed! Check LangSmith for detailed results.\")\n",
    "    print(f\"ðŸ“Š Evaluation Results Summary: {evaluation_results}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Evaluation encountered an issue: {e}\")\n",
    "    print(\"ðŸ’¡ This may be due to API rate limits or network issues.\")\n",
    "    print(\"ðŸ”„ You can check the LangSmith dashboard for partial results.\")\n",
    "    print(f\"ðŸŒ Dataset URL: https://smith.langchain.com/datasets/{langsmith_dataset_obj.id}\")\n",
    "    \n",
    "    # Set a placeholder result to continue the workflow\n",
    "    evaluation_results = {\"status\": \"partial\", \"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Our RAG Application\n",
    "\n",
    "Let's make improvements to our RAG chain based on evaluation insights and test it again on our **LangGraph synthetic data**!\n",
    "\n",
    "We'll implement:\n",
    "- Include an empathy-enhanced prompt  \n",
    "- Use larger chunks for better context\n",
    "- Improve the retriever model to: `text-embedding-3-large`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced RAG implementation\n",
    "EMPATHY_RAG_PROMPT = \"\"\"\\\n",
    "Given a provided context and question, you must answer the question based only on context.\n",
    "\n",
    "If you cannot answer the question based on the context - you must say \"I don't know\".\n",
    "\n",
    "You must answer the question using empathy and kindness, and make sure the user feels heard.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "empathy_rag_prompt = ChatPromptTemplate.from_template(EMPATHY_RAG_PROMPT)\n",
    "\n",
    "# Use larger chunks for better context\n",
    "rag_documents_improved = documents\n",
    "\n",
    "text_splitter_improved = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,  # Increased from 500\n",
    "    chunk_overlap = 50\n",
    ")\n",
    "\n",
    "rag_documents_improved = text_splitter_improved.split_documents(rag_documents_improved)\n",
    "\n",
    "# Use better embeddings\n",
    "embeddings_improved = OpenAIEmbeddings(model=\"text-embedding-3-large\")  # Upgraded\n",
    "\n",
    "vectorstore_improved = Qdrant.from_documents(\n",
    "    documents=rag_documents_improved,\n",
    "    embedding=embeddings_improved,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"LangGraph RAG Improved\"\n",
    ")\n",
    "\n",
    "retriever_improved = vectorstore_improved.as_retriever(search_kwargs={\"k\": 8})\n",
    "\n",
    "# Create improved RAG chain\n",
    "empathy_rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever_improved, \"question\": itemgetter(\"question\")}\n",
    "    | empathy_rag_prompt | rag_llm | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Test the improved chain\n",
    "test_result_improved = empathy_rag_chain.invoke({\"question\" : \"What kinds of loans are available?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can evaluate our **improved chain** on the same **LangGraph synthetic test set**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting evaluation of improved RAG chain...\n",
      "View the evaluation results for experiment: 'memorable-river-30' at:\n",
      "https://smith.langchain.com/o/3c2c7006-57b9-4cbe-911e-6f73b4734883/datasets/dc35309b-1930-44d7-badd-9603b8f35f2e/compare?selectedSessions=3599fc74-08bf-486f-9b46-e4abd346473e\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d56acc6ada4d7a82eea30863fec919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Improved chain evaluation completed!\n",
      "ðŸ“Š Compare results in LangSmith to see improvements!\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸš€ Starting evaluation of improved RAG chain...\")\n",
    "\n",
    "try:\n",
    "    # Run evaluation on improved chain with better settings\n",
    "    evaluation_results_improved = evaluate(\n",
    "        empathy_rag_chain.invoke,\n",
    "        data=dataset_name,\n",
    "        evaluators=[\n",
    "            qa_evaluator,\n",
    "            labeled_helpfulness_evaluator,\n",
    "            empathy_evaluator\n",
    "        ],\n",
    "        metadata={\"revision_id\": \"langgraph_empathy_rag_chain\", \"data_source\": \"LangGraph-EvolInstruct\"},\n",
    "        max_concurrency=1,  # Limit concurrency to prevent hanging\n",
    "        client=client       # Explicit client\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâœ… Improved chain evaluation completed!\")\n",
    "    print(f\"ðŸ“Š Compare results in LangSmith to see improvements!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Improved evaluation encountered an issue: {e}\")\n",
    "    print(\"ðŸ’¡ This may be due to API rate limits or network issues.\")\n",
    "    print(\"ðŸ”„ You can check the LangSmith dashboard for partial results.\")\n",
    "    print(f\"ðŸŒ Dataset URL: https://smith.langchain.com/datasets/{langsmith_dataset_obj.id}\")\n",
    "    \n",
    "    # Set a placeholder result to continue the workflow\n",
    "    evaluation_results_improved = {\"status\": \"partial\", \"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"assets/Langgraph_SDG_evaluation.png\" style=\"width:100%;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
